# 爬虫 #

## 1.什么是爬虫 ##
要说爬虫就得先说，**互联网**，互联网是由一个个站点，和网络设备组成的一个巨大的网络，我们通过浏览器访问站点，站点把HTML，JS,CSS代码返回给浏览器。浏览器解析，渲染这些代码将各种页面呈现在给我们。<br>
若是将互联网比作一张蜘蛛网，数据就是存放在蜘蛛网的各个节点，**爬虫**就是一个 小蜘蛛。沿着网络抓取自己的猎物（数据）爬虫指的是：向网站发起请求，获取资源后分析并提取有用数据的程序；<br>
从技术层面来说就是 通过程序模拟浏览器请求站点的行为，把站点返回的HTML代码/JSON数据/二进制数据（图片、视频） 爬到本地，进而提取自己需要的数据，存放起来使用；<br>
![](https://i.imgur.com/DOOdV9V.png)

## 2.爬虫爬取数据的基本流程 ##
用户获取网络数据的方式：<br>
方式1：浏览器提交请求--->下载网页代码--->解析成页面<br>
方式2：模拟浏览器发送请求(获取网页代码)->提取有用的数据->存放于数据库或文件中<br>
爬虫要做的就是方式2；<br>
**发送请求-->获取响应内同-->解析内容-->保存数据** <br>

(1)、发起请求<br>
使用http库向目标站点发起请求，即发送一个Request<be>
Request包含：请求头、请求体等 <br>
Request模块缺陷：不能执行JS 和CSS 代码<br>

(2)、获取响应内容<br>
如果服务器能正常响应，则会得到一个Response<br>
Response包含：html，json，图片，视频等<br>
(3)、解析内容<br>
解析html数据：正则表达式（RE模块），第三方解析库如eautifulsoup，pyquery等<br>
解析json数据：json模块<br>
解析二进制数据:以wb的方式写入文件<br>
(4)、保存数据<br>
数据库（MySQL，Mongdb、Redis）文件<br>


## 3.http 的请求与响应 ##
**Request：**用户将自己的信息通过浏览器（socket client）发送给服务器（socket server）<br>
**Response：**服务器接收请求，分析用户发来的请求信息，然后返回数据（返回的数据中可能包含其他链接，如：图片，js，css等）<br>
**ps：**浏览器在接收Response后，会解析其内容来显示给用户，而爬虫程序在模拟浏览器发送请求然后接收Response后，是要提取其中的有用数据。<br>
![](https://i.imgur.com/skaFFOX.png)

## 4. request  ##
**请求方式：** 常见的请求方式为post/get;
**URL：**统一资源定位符，用来定义互联网上的一个唯一的资源（图片，文件，视屏等等）

**请求头：**<br>
User-agent：请求头中如果没有user-agent客户端配置，服务端可能将你当做一个非法用户host；<br>
cookies：cookie用来保存登录信息<br>
注意： 一般做爬虫都会加上请求头<br>

![](https://i.imgur.com/qrmxEgi.png)

![](https://i.imgur.com/CayDs9a.png)

![](https://i.imgur.com/QWNXQno.png)

请求头需要注意的参数：<br>
（1）Referrer：访问源至哪里来（一些大型网站，会通过Referrer 做防盗链策略；所有爬虫也要注意模拟）<br>
（2）User-Agent:访问的浏览器（要加上否则会被当成爬虫程序）<br>
（3）cookie：请求头注意携带<br>

**请求体：**<br>
请求体<br>
如果是get方式，请求体没有内容 （get请求的请求体放在 url后面参数中，直接能看到）<br>
如果是post方式，请求体是format data <br>
ps：1、登录窗口，文件上传等，信息都会被附加到请求体内<br>
2、登录，输入错误的用户名密码，然后提交，就可以看到post，正确登录后页面通常会跳转，无法捕捉到post<br>

## 5.响应 response
**响应状态码：**<br>

    200：代表成功
    301：代表跳转
    404：文件不存在
    403：无权限访问
    502：服务器错误

**response header:**<br>
响应头需要注意的参数：<br>
（1）Set-Cookie:BDSVRTM=0; path=/：可能有多个，是来告诉浏览器，把cookie保存下来<br>
（2）Content-Location：服务端响应头中包含Location返回浏览器之后，浏览器就会重新访问另一个页面<br>
**preview:**就是网页源代码
JSO数据<br>
如网页html，图片<br>
二进制数据等 <br>


##6.  举个例子 ##

爬取校花网上的数据：<br>
**流程：** 爬取--->解析--->存储<br>
**所需工具：**<br>
请求库：requests,selenium（可以驱动浏览器解析渲染CSS和JS，但有性能劣势（有用没用的网页都会加载）；）<br>
解析库：正则，beautifulsoup，pyquery <br>
存储库：文件，MySQL，Mongodb，Redis <br>

**LEVLE 1(基础版):** <br>
    import re
    import requests
    
    respose=requests.get('http://www.xiaohuar.com/v/')
    # print(respose.status_code)# 响应的状态码
    # print(respose.content)  #返回字节信息
    # print(respose.text)  #返回文本内容
    urls=re.findall(r'class="items".*?href="(.*?)"',respose.text,re.S)  #re.S 把文本信息转换成1行匹配
    url=urls[5]
    result=requests.get(url)
    mp4_url=re.findall(r'id="media".*?src="(.*?)"',result.text,re.S)[0]
    
    video=requests.get(mp4_url)
    
    with open('D:\\a.mp4','wb') as f:
    f.write(video.content)


**LEVEL 2（函数封装版):**
    import re
    import requests
    import hashlib
    import time
    
    # respose=requests.get('http://www.xiaohuar.com/v/')
    # # print(respose.status_code)# 响应的状态码
    # # print(respose.content)  #返回字节信息
    # # print(respose.text)  #返回文本内容
    # urls=re.findall(r'class="items".*?href="(.*?)"',respose.text,re.S)  #re.S 把文本信息转换成1行匹配
    # url=urls[5]
    # result=requests.get(url)
    # mp4_url=re.findall(r'id="media".*?src="(.*?)"',result.text,re.S)[0]
    #
    # video=requests.get(mp4_url)
    #
    # with open('D:\\a.mp4','wb') as f:
    # f.write(video.content)
    #
    
    
    def get_index(url):
	    respose = requests.get(url)
	    if respose.status_code==200:
	    	return respose.text
    
    def parse_index(res):
	    urls = re.findall(r'class="items".*?href="(.*?)"', res,re.S)  # re.S 把文本信息转换成1行匹配
	    return urls
    
    
    def get_detail(urls):
	    for url in urls:
	    if not url.startswith('http'):
		    url='http://www.xiaohuar.com%s' %url
		    result = requests.get(url)
		    if result.status_code==200 :
		    mp4_url_list = re.findall(r'id="media".*?src="(.*?)"', result.text, re.S)
	    if mp4_url_list:
		    mp4_url=mp4_url_list[0]
		    print(mp4_url)
    		# save(mp4_url)
    
    
    def save(url):
    	video = requests.get(url)
    	if video.status_code==200:
	    m=hashlib.md5()
	    m.updata(url.encode('utf-8'))
	    m.updata(str(time.time()).encode('utf-8'))
	    filename=r'%s.mp4'% m.hexdigest()
	    filepath=r'D:\\%s'%filename
	    with open(filepath, 'wb') as f:
	    f.write(video.content)
    
    def main():
    	for i in range(5):
    	res1 = get_index('http://www.xiaohuar.com/list-3-%s.html'% i )
    	res2 = parse_index(res1)
    	get_detail(res2)
    
    if __name__ == '__main__':
    	main()


**LEVEL 3（多线程访问版):**

    import re
    import requests
    import hashlib
    import time
    from concurrent.futures import ThreadPoolExecutor
    p=ThreadPoolExecutor(30) #创建1个程池中，容纳线程个数为30个；
    
    
    def get_index(url):
	    respose = requests.get(url)
	    if respose.status_code==200:
	    	return respose.text
    
    def parse_index(res):
	    res=res.result() #进程执行完毕后，得到1个对象
	    urls = re.findall(r'class="items".*?href="(.*?)"', res,re.S)  # re.S 把文本信息转换成1行匹配
	    for url in urls:
	    	p.submit(get_detail(url))  #获取详情页 提交到线程池
    
    
    
    def get_detail(url):  #只下载1个视频
	    if not url.startswith('http'):
	    	url='http://www.xiaohuar.com%s' %url
	    	result = requests.get(url)
	    if result.status_code==200 :
	   		mp4_url_list = re.findall(r'id="media".*?src="(.*?)"', result.text, re.S)
	    if mp4_url_list:
	    	mp4_url=mp4_url_list[0]
	    	print(mp4_url)
	    	# save(mp4_url)
	    
    
    def save(url):
    	video = requests.get(url)
    	if video.status_code==200:
    		m=hashlib.md5()
    		m.updata(url.encode('utf-8'))
    		m.updata(str(time.time()).encode('utf-8'))
    		filename=r'%s.mp4'% m.hexdigest()
    		filepath=r'D:\\%s'%filename
    		with open(filepath, 'wb') as f:
    		f.write(video.content)
    
    def main():
    	for i in range(5):
    		p.submit(get_index,'http://www.xiaohuar.com/list-3-%s.html'% i ).add_done_callback(parse_index)
		    #1、先把爬主页的任务（get_index）异步提交到线程池
		    #2、get_index任务执行完后，会通过回调函add_done_callback（）数通知主线程，任务完成；
		    #2、把get_index执行结果（注意线程执行结果是对象，调用res=res.result()方法，才能获取真正执行结果），当做参数传给parse_index
		    #3、parse_index任务执行完毕后，
		    #4、通过循环，再次把获取详情页 get_detail（）任务提交到线程池执行
    if __name__ == '__main__':
    	main()

